{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYJsLJkhAT4tWQXyy0cGnQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Cross-encoders concatenate both inputs into a single sequence and process them jointly through a transformer, allowing full token-to-token attention for richer interactions. They output a direct relevance score but require processing each pair anew, making them slower.\n",
        "\n",
        "###Why Reranking?\n",
        "We use reranking because initial retrieval with embeddings is fast but not very precise. Reranking with a cross-encoder improves relevance by jointly analyzing the query and documents, which leads to more accurate results for the final LLM response.\n",
        "\n",
        "###Can you give an example?\n",
        "\n",
        "You: Sure. Query: \"How does reranking improve RAG?\" A bi-encoder might rank a doc about \"RAG music festivals\" high because of keyword overlap, while burying the perfect technical explanation. Reranking uses cross-encoders to score query-document pairs deeply, promoting truly relevant docs to top-5 for the LLM.\n",
        "\n",
        "###What are the benefits?\n",
        "\n",
        "You: 10-30% hit rate improvement, noise reduction, and better LLM context. Retrieval is RAG's biggest bottleneckâ€”reranking fixes that without sacrificing scale, since you only rerank a small candidate set."
      ],
      "metadata": {
        "id": "ti5cG9VHNnyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGoIcDUcNbX1"
      },
      "outputs": [],
      "source": []
    }
  ]
}